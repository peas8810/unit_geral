import os
import uuid
import tempfile
import io
import streamlit as st
# PDF utilities
from PyPDF2 import PdfMerger, PdfReader, PdfWriter
from pdf2docx import Converter
from PIL import Image
import pytesseract
from pdf2image import convert_from_path
import img2pdf
# Plagiarism detection
import requests
import difflib
from fpdf import FPDF
import hashlib
from datetime import datetime
# Cita√ß√£o IA
import random
import pdfplumber
from collections import Counter
import nltk
from nltk.corpus import stopwords
# Total IA
import re
import numpy as np
import pandas as pd
from transformers import RobertaTokenizer, RobertaForSequenceClassification
import torch
# LaTeX generation
import pypandoc
from jinja2 import Environment, FileSystemLoader

# --- CONFIGURA√á√ïES GERAIS ---
st.set_page_config(page_title="Portal PEAS.Co", layout="wide")
WORK_DIR = "documentos"
os.makedirs(WORK_DIR, exist_ok=True)
# Ajuste do Tesseract (se necess√°rio)
pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'

# Fun√ß√µes de cada m√≥dulo (resumidas) ...
# TODO: Copiar aqui as fun√ß√µes pdf_para_word, juntar_pdf, dividir_pdf, jpg_para_pdf
# Configura√ß√µes iniciais
WORK_DIR = "documentos"
os.makedirs(WORK_DIR, exist_ok=True)

# Verifica e configura o Tesseract OCR (pode precisar de ajuste no seu sistema)
pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'

# Fun√ß√£o para salvar arquivos enviados
def salvar_arquivos(uploaded_files):
    caminhos = []
    for uploaded_file in uploaded_files:
        # Limpa o nome do arquivo
        nome_base, extensao = os.path.splitext(uploaded_file.name)
        nome_limpo = (nome_base.replace(" ", "_")
                      .replace("√ß", "c").replace("√£", "a")
                      .replace("√°", "a").replace("√©", "e")
                      .replace("√≠", "i").replace("√≥", "o")
                      .replace("√∫", "u").replace("√±", "n")) + extensao.lower()
        
        caminho = os.path.join(WORK_DIR, nome_limpo)
        with open(caminho, "wb") as f:
            f.write(uploaded_file.getbuffer())
        caminhos.append(caminho)
    return caminhos

# Fun√ß√£o para limpar arquivos tempor√°rios
def limpar_diretorio():
    for filename in os.listdir(WORK_DIR):
        file_path = os.path.join(WORK_DIR, filename)
        try:
            if os.path.isfile(file_path):
                os.unlink(file_path)
        except Exception as e:
            st.error(f"Erro ao limpar arquivo {file_path}: {e}")

# Fun√ß√£o para baixar arquivos
def criar_link_download(nome_arquivo, label):
    with open(os.path.join(WORK_DIR, nome_arquivo), "rb") as f:
        st.download_button(
            label=label,
            data=f,
            file_name=nome_arquivo,
            mime="application/octet-stream"
        )

# Fun√ß√µes de convers√£o
def pdf_para_word():
    st.header("PDF para Word")
    uploaded_file = st.file_uploader(
        "Carregue um arquivo PDF",
        type=["pdf"],
        accept_multiple_files=False
    )
    
    if uploaded_file and st.button("Converter para Word"):
        caminho = salvar_arquivos([uploaded_file])[0]
        nome_base = os.path.splitext(os.path.basename(caminho))[0]
        nome_saida = f"pdf2docx_{nome_base}.docx"
        saida = os.path.join(WORK_DIR, nome_saida)
        
        # Remove arquivo existente se houver
        if os.path.exists(saida):
            os.remove(saida)
        
        try:
            cv = Converter(caminho)
            cv.convert(saida)
            cv.close()
            
            if os.path.exists(saida):
                st.success(f"Arquivo convertido: {nome_saida}")
                criar_link_download(nome_saida, f"Baixar {nome_saida}")
            else:
                st.error(f"Falha na convers√£o: {caminho}")
        except Exception as e:
            st.error(f"Erro na convers√£o: {str(e)}")

def juntar_pdf():
    st.header("Juntar PDFs")
    uploaded_files = st.file_uploader(
        "Carregue os PDFs para juntar",
        type=["pdf"],
        accept_multiple_files=True
    )
    
    if len(uploaded_files) >= 2 and st.button("Juntar PDFs"):
        caminhos = salvar_arquivos(uploaded_files)
        nome_saida = "merge_resultado.pdf"
        saida = os.path.join(WORK_DIR, nome_saida)
        
        # Remove arquivo existente se houver
        if os.path.exists(saida):
            os.remove(saida)
        
        try:
            merger = PdfMerger()
            for c in caminhos:
                merger.append(c)
            merger.write(saida)
            merger.close()
            
            if os.path.exists(saida):
                st.success("PDFs unidos com sucesso!")
                criar_link_download(nome_saida, f"Baixar {nome_saida}")
            else:
                st.error("Falha ao unir PDFs.")
        except Exception as e:
            st.error(f"Erro ao unir PDFs: {str(e)}")

def dividir_pdf():
    st.header("Dividir PDF")
    uploaded_file = st.file_uploader(
        "Carregue um PDF para dividir",
        type=["pdf"],
        accept_multiple_files=False
    )
    
    if uploaded_file and st.button("Dividir PDF"):
        caminho = salvar_arquivos([uploaded_file])[0]
        nome_base = os.path.splitext(os.path.basename(caminho))[0]
        
        try:
            reader = PdfReader(caminho)
            for i, page in enumerate(reader.pages):
                writer = PdfWriter()
                writer.add_page(page)
                nome_saida = f"split_{nome_base}_pag{i+1}.pdf"
                out_path = os.path.join(WORK_DIR, nome_saida)
                with open(out_path, "wb") as f:
                    writer.write(f)
                st.success(f"P√°gina gerada: {nome_saida}")
                criar_link_download(nome_saida, f"Baixar {nome_saida}")
        except Exception as e:
            st.error(f"Erro ao dividir PDF: {str(e)}")

def jpg_para_pdf():
    st.header("Imagens para PDF")
    uploaded_files = st.file_uploader(
        "Carregue imagens para converter em PDF",
        type=["jpg", "jpeg", "png"],
        accept_multiple_files=True
    )
    
    if uploaded_files and st.button("Converter para PDF"):
        caminhos = salvar_arquivos(uploaded_files)
        nome_saida = "img2pdf_resultado.pdf"
        caminho_pdf = os.path.join(WORK_DIR, nome_saida)
        
        # Remove arquivo existente se houver
        if os.path.exists(caminho_pdf):
            os.remove(caminho_pdf)
        
        try:
            # Usando img2pdf para melhor qualidade
            with open(caminho_pdf, "wb") as f:
                f.write(img2pdf.convert([Image.open(img).filename for img in caminhos]))
            
            if os.path.exists(caminho_pdf):
                st.success("PDF gerado com sucesso!")
                criar_link_download(nome_saida, f"Baixar {nome_saida}")
            else:
                st.error("Falha ao gerar PDF.")
        except Exception as e:
            st.error(f"Erro ao converter imagens para PDF: {str(e)}")


# Interface principal
def main():
    st.title("üìÑ Conversor de Documentos")
    st.markdown("""
    Ferramenta para convers√£o entre diversos formatos de documentos.
    """)
    
    # Menu lateral
    st.sidebar.title("Menu")
    opcao = st.sidebar.selectbox(
        "Selecione a opera√ß√£o",
        [
            "PDF para Word",
            "Juntar PDFs",
            "Dividir PDF",
            "Imagens para PDF",
         ]
    )
    
    # Limpar arquivos tempor√°rios
    if st.sidebar.button("Limpar arquivos tempor√°rios"):
        limpar_diretorio()
        st.sidebar.success("Arquivos tempor√°rios removidos!")
    
    # Rodap√©
    st.sidebar.markdown("---")
    st.sidebar.markdown("Desenvolvido por PEAS.Co")
    
    # Executa a fun√ß√£o selecionada
    if opcao == "PDF para Word":
        pdf_para_word()
    elif opcao == "Juntar PDFs":
        juntar_pdf()
    elif opcao == "Dividir PDF":
        dividir_pdf()
    elif opcao == "Imagens para PDF":
        jpg_para_pdf()


if __name__ == "__main__":
    main()


# --- Se√ß√£o de Propaganda ---

# Incorpora√ß√£o de website (exemplo de iframe para propaganda)
st.markdown(
    "<h3><a href='https://peas8810.hotmart.host/product-page-1f2f7f92-949a-49f0-887c-5fa145e7c05d' target='_blank'>"
    "T√©cnica PROATIVA: Domine a Cria√ß√£o de Comandos Poderosos na IA e gere produtos monetiz√°veis"
    "</a></h3>",
    unsafe_allow_html=True
)
st.components.v1.iframe("https://pay.hotmart.com/U99934745U?off=y2b5nihy&hotfeature=51&_hi=eyJjaWQiOiIxNzQ4Mjk4OTUxODE2NzQ2NTc3ODk4OTY0NzUyNTAwIiwiYmlkIjoiMTc0ODI5ODk1MTgxNjc0NjU3Nzg5ODk2NDc1MjUwMCIsInNpZCI6ImM4OTRhNDg0MzJlYzRhZTk4MTNjMDJiYWE2MzdlMjQ1In0=.1748375599003&bid=1748375601381", height=250)


# TODO: Fun√ß√µes de PlagIA (salvar_email, verificar_codigo, extrair_texto, calcular_similaridade, buscar_referencias, gerar_relatorio)
# üîó URL da API gerada no Google Sheets
URL_GOOGLE_SHEETS = "https://script.google.com/macros/s/AKfycbyTpbWDxWkNRh_ZIlHuAVwZaCC2ODqTmo0Un7ZDbgzrVQBmxlYYKuoYf6yDigAPHZiZ/exec"

# =============================
# üìã Fun√ß√£o para Salvar E-mails e C√≥digo de Verifica√ß√£o no Google Sheets
# =============================
def salvar_email_google_sheets(nome, email, codigo_verificacao):
    dados = {
        "nome": nome,
        "email": email,
        "codigo": codigo_verificacao
    }
    try:
        headers = {'Content-Type': 'application/json'}
        response = requests.post(URL_GOOGLE_SHEETS, json=dados, headers=headers)

        if response.text.strip() == "Sucesso":
            st.success("‚úÖ E-mail, nome e c√≥digo registrados com sucesso!")
        else:
            st.error(f"‚ùå Erro ao salvar dados no Google Sheets: {response.text}")
    except Exception as e:
        st.error(f"‚ùå Erro na conex√£o com o Google Sheets: {e}")

# =============================
# üîé Fun√ß√£o para Verificar C√≥digo de Verifica√ß√£o na Planilha
# =============================
def verificar_codigo_google_sheets(codigo_digitado):
    try:
        response = requests.get(f"{URL_GOOGLE_SHEETS}?codigo={codigo_digitado}")
        if response.text.strip() == "Valido":
            return True
        else:
            return False
    except Exception as e:
        st.error(f"‚ùå Erro na conex√£o com o Google Sheets: {e}")
        return False

# =============================
# üîê Fun√ß√£o para Gerar C√≥digo de Verifica√ß√£o
# =============================
def gerar_codigo_verificacao(texto):
    return hashlib.md5(texto.encode()).hexdigest()[:10].upper()

# =============================
# üìù Fun√ß√£o para Extrair Texto do PDF
# =============================
def extrair_texto_pdf(arquivo_pdf):
    leitor_pdf = PyPDF2.PdfReader(arquivo_pdf)
    texto = ""
    for pagina in leitor_pdf.pages:
        texto += pagina.extract_text() or ""
    return texto.strip()

# =============================
# üìä Fun√ß√£o para Calcular Similaridade
# =============================
def calcular_similaridade(texto1, texto2):
    seq_matcher = difflib.SequenceMatcher(None, texto1, texto2)
    return seq_matcher.ratio()

# =============================
# üîé Fun√ß√£o para Buscar Artigos na API CrossRef
# =============================
def buscar_referencias_crossref(texto):
    query = "+".join(texto.split()[:10])  
    url = f"https://api.crossref.org/works?query={query}&rows=10"

    try:
        response = requests.get(url)
        response.raise_for_status()
        data = response.json()
    except requests.exceptions.RequestException as e:
        st.error(f"Erro ao acessar a API da CrossRef: {e}")
        return []

    referencias = []
    for item in data.get("message", {}).get("items", []):
        titulo = item.get("title", ["T√≠tulo n√£o dispon√≠vel"])[0]
        resumo = item.get("abstract", "")
        link = item.get("URL", "Link n√£o dispon√≠vel")
        referencias.append({"titulo": titulo, "resumo": resumo, "link": link})

    return referencias

# =============================
# üìÑ Classe para Gerar Relat√≥rio PDF Personalizado
# =============================
class PDF(FPDF):
    def header(self):
        self.set_font('Arial', 'B', 16)
        self.cell(0, 10, "Relat√≥rio de Similaridade de Pl√°gio - PlagIA - PEAS.Co", ln=True, align='C')

    def chapter_title(self, title):
        self.set_font('Arial', 'B', 12)
        self.cell(0, 10, self._encode_text(title), ln=True)
        self.ln(3)

    def chapter_body(self, body):
        self.set_font('Arial', '', 10)
        self.multi_cell(0, 8, self._encode_text(body))
        self.ln()

    # üîé Fun√ß√£o para corrigir acentua√ß√£o e caracteres especiais
    def _encode_text(self, text):
        try:
            return text.encode('latin-1', 'replace').decode('latin-1')
        except UnicodeEncodeError:
            return ''.join(char if ord(char) < 128 else '?' for char in text)

def gerar_relatorio_pdf(referencias_com_similaridade, nome, email, codigo_verificacao):
    pdf = PDF()
    pdf.add_page()

    # Adicionando os dados do usu√°rio no PDF
    data_hora = datetime.now().strftime("%d/%m/%Y %H:%M:%S")
    pdf.chapter_title("Dados do Solicitante:")
    pdf.chapter_body(f"Nome: {nome}")
    pdf.chapter_body(f"E-mail: {email}")
    pdf.chapter_body(f"Data e Hora: {data_hora}")
    pdf.chapter_body(f"C√≥digo de Verifica√ß√£o (Link para inserir o c√≥digo e conferir: https://iaplagio-wtwg4f3x2ejse4rspbqe2s.streamlit.app/): {codigo_verificacao}")
    
    # Refer√™ncias encontradas
    pdf.chapter_title("Top Refer√™ncias encontradas:")
    soma_percentual = 0
    # Considera no m√°ximo 5 refer√™ncias, mas se houver menos, divide pelo n√∫mero real
    refs_selecionadas = referencias_com_similaridade[:5]
    num_refs = len(refs_selecionadas)
    if num_refs == 0:
        pdf.chapter_body("Nenhuma refer√™ncia encontrada.")
    else:
        for i, (ref, perc, link) in enumerate(refs_selecionadas, 1):
            soma_percentual += perc
            pdf.chapter_body(f"{i}. {ref} - {perc*100:.2f}%\n{link}")
        plagio_medio = (soma_percentual / num_refs) * 100
        pdf.chapter_body(f"Pl√°gio m√©dio: {plagio_medio:.2f}%")

    pdf_file_path = "/tmp/relatorio_plagio.pdf"
    pdf.output(pdf_file_path, 'F')
      
    return pdf_file_path

# =============================
# üíª Interface do Streamlit
# =============================
if __name__ == "__main__":
    st.title("PlagIA - PEAS.Co")
    
    # --- Registro de Usu√°rio ---
    st.subheader("üìã Registro de Usu√°rio - Apenas para valida√ß√£o")
    nome = st.text_input("Nome completo")
    email = st.text_input("E-mail")

    if st.button("Salvar Dados"):
        if nome and email:
            salvar_email_google_sheets(nome, email, "N/A")
        else:
            st.warning("‚ö†Ô∏è Por favor, preencha todos os campos.")

    # --- Upload e Processamento do PDF ---
    arquivo_pdf = st.file_uploader("Fa√ßa upload de um arquivo PDF SEM OS NOMES DOS AUTORES E T√çTULO DA REVISTA, PARA GARANTIR AVALIA√á√ÉO SOMENTE DO TEXTO", type=["pdf"])

    if st.button("Processar PDF"):
        if arquivo_pdf is not None:
            texto_usuario = extrair_texto_pdf(arquivo_pdf)
            referencias = buscar_referencias_crossref(texto_usuario)

            referencias_com_similaridade = []
            for ref in referencias:
                texto_base = ref["titulo"] + " " + ref["resumo"]
                link = ref["link"]
                similaridade = calcular_similaridade(texto_usuario, texto_base)
                referencias_com_similaridade.append((ref["titulo"], similaridade, link))

            referencias_com_similaridade.sort(key=lambda x: x[1], reverse=True)

            if referencias_com_similaridade:
                codigo_verificacao = gerar_codigo_verificacao(texto_usuario)
                salvar_email_google_sheets(nome, email, codigo_verificacao)
                st.success(f"C√≥digo de verifica√ß√£o gerado: **{codigo_verificacao}**")

                # Gerar e exibir link para download do relat√≥rio
                pdf_file = gerar_relatorio_pdf(referencias_com_similaridade, nome, email, codigo_verificacao)
                with open(pdf_file, "rb") as f:
                    st.download_button("üìÑ Baixar Relat√≥rio de Pl√°gio", f, "relatorio_plagio.pdf")
            else:
                st.warning("Nenhuma refer√™ncia encontrada.")
        else:
            st.error("Por favor, carregue um arquivo PDF.")

    # --- Texto Justificado ---
    st.markdown(
        """
        <div style="text-align: justify;">
        Powered By - PEAS.Co
        </div>
        """, 
        unsafe_allow_html=True
    )

    # --- Verifica√ß√£o de C√≥digo ---
    st.header("Verificar Autenticidade")
    codigo_digitado = st.text_input("Digite o c√≥digo de verifica√ß√£o:")

    if st.button("Verificar C√≥digo"):
        if verificar_codigo_google_sheets(codigo_digitado):
            st.success("‚úÖ Documento Aut√™ntico e Original!")
        else:
            st.error("‚ùå C√≥digo inv√°lido ou documento falsificado.")

    # --- Se√ß√£o de Propaganda ---

      # Incorpora√ß√£o de website (exemplo de iframe para propaganda)
st.markdown(
    "<h3><a href='https://peas8810.hotmart.host/product-page-1f2f7f92-949a-49f0-887c-5fa145e7c05d' target='_blank'>"
    "T√©cnica PROATIVA: Domine a Cria√ß√£o de Comandos Poderosos na IA e gere produtos monetiz√°veis"
    "</a></h3>",
    unsafe_allow_html=True
)
st.components.v1.iframe("https://pay.hotmart.com/U99934745U?off=y2b5nihy&hotfeature=51&_hi=eyJjaWQiOiIxNzQ4Mjk4OTUxODE2NzQ2NTc3ODk4OTY0NzUyNTAwIiwiYmlkIjoiMTc0ODI5ODk1MTgxNjc0NjU3Nzg5ODk2NDc1MjUwMCIsInNpZCI6ImM4OTRhNDg0MzJlYzRhZTk4MTNjMDJiYWE2MzdlMjQ1In0=.1748375599003&bid=1748375601381", height=250)




# TODO: Fun√ß√µes de CitatIA (salvar_email, verificar_codigo, gerar_codigo, get_popular_phrases, extract_top_keywords, get_publication_statistics, evaluate_article_relevance, generate_report)
nltk.download('stopwords')

STOP_WORDS = set(stopwords.words('portuguese'))

# üîó URL da API do Google Sheets
URL_GOOGLE_SHEETS = "https://script.google.com/macros/s/AKfycbyHRCrD5-A_JHtaUDXsGWQ22ul9ml5vvK3YYFzIE43jjCdip0dBMFH_Jmd8w971PLte/exec"

# URLs das APIs
SEMANTIC_API = "https://api.semanticscholar.org/graph/v1/paper/search"
CROSSREF_API = "https://api.crossref.org/works"

# =============================
# üìã Fun√ß√£o para Salvar E-mails e C√≥digo de Verifica√ß√£o no Google Sheets
# =============================
def salvar_email_google_sheets(nome, email, codigo_verificacao):
    dados = {
        "nome": nome,
        "email": email,
        "codigo": codigo_verificacao
    }
    try:
        headers = {'Content-Type': 'application/json'}
        response = requests.post(URL_GOOGLE_SHEETS, json=dados, headers=headers)
        if response.text.strip() == "Sucesso":
            st.success("‚úÖ E-mail, nome e c√≥digo registrados com sucesso!")
        else:
            st.error(f"‚ùå Erro ao salvar dados no Google Sheets: {response.text}")
    except Exception as e:
        st.error(f"‚ùå Erro na conex√£o com o Google Sheets: {e}")

# =============================
# üîé Fun√ß√£o para Verificar C√≥digo de Verifica√ß√£o na Planilha
# =============================
def verificar_codigo_google_sheets(codigo_digitado):
    try:
        response = requests.get(f"{URL_GOOGLE_SHEETS}?codigo={codigo_digitado}")
        if response.text.strip() == "Valido":
            return True
        else:
            return False
    except Exception as e:
        st.error(f"‚ùå Erro na conex√£o com o Google Sheets: {e}")
        return False

# =============================
# üîê Fun√ß√£o para Gerar C√≥digo de Verifica√ß√£o
# =============================
def gerar_codigo_verificacao(texto):
    return hashlib.md5(texto.encode()).hexdigest()[:10].upper()

# Fun√ß√£o para obter artigos mais citados
def get_popular_phrases(query, limit=10):
    suggested_phrases = []

    # Pesquisa na API Semantic Scholar
    semantic_params = {"query": query, "limit": limit, "fields": "title,abstract,url,externalIds,citationCount"}
    semantic_response = requests.get(SEMANTIC_API, params=semantic_params)
    if semantic_response.status_code == 200:
        semantic_data = semantic_response.json().get("data", [])
        for item in semantic_data:
            suggested_phrases.append({
                "phrase": f"{item.get('title', '')}. {item.get('abstract', '')}",
                "doi": item['externalIds'].get('DOI', 'N/A'),
                "link": item.get('url', 'N/A'),
                "citationCount": item.get('citationCount', 0)
            })

    # Pesquisa na API CrossRef
    crossref_params = {"query": query, "rows": limit}
    crossref_response = requests.get(CROSSREF_API, params=crossref_params)
    if crossref_response.status_code == 200:
        crossref_data = crossref_response.json().get("message", {}).get("items", [])
        for item in crossref_data:
            suggested_phrases.append({
                "phrase": f"{item.get('title', [''])[0]}. {item.get('abstract', '')}",
                "doi": item.get('DOI', 'N/A'),
                "link": item.get('URL', 'N/A'),
                "citationCount": item.get('is-referenced-by-count', 0)
            })

    # Ordenar por n√∫mero de cita√ß√µes
    suggested_phrases.sort(key=lambda x: x.get('citationCount', 0), reverse=True)

    return suggested_phrases

# Fun√ß√£o para extrair as 10 palavras mais importantes dos artigos
def extract_top_keywords(suggested_phrases):
    all_text = " ".join([item['phrase'] for item in suggested_phrases])
    words = re.findall(r'\b\w+\b', all_text.lower())
    words = [word for word in words if word not in STOP_WORDS and len(word) > 3]  # Filtra stopwords e palavras curtas
    word_freq = Counter(words).most_common(10)
    return [word for word, freq in word_freq]

# Fun√ß√£o para simular estat√≠sticas de publica√ß√µes mensais
def get_publication_statistics(total_articles):
    start_date = datetime.now() - timedelta(days=365)  # √öltimo ano
    publication_dates = [start_date + timedelta(days=random.randint(0, 365)) for _ in range(total_articles)]
    monthly_counts = Counter([date.strftime("%Y-%m") for date in publication_dates])
    proportion_per_100 = (total_articles / 100) * 100  # Normaliza para 100
    return monthly_counts, proportion_per_100

# Modelo PyTorch para prever chance de ser refer√™ncia
class ArticlePredictor(nn.Module):
    def __init__(self):
        super(ArticlePredictor, self).__init__()
        self.fc1 = nn.Linear(1, 16)
        self.fc2 = nn.Linear(16, 8)
        self.fc3 = nn.Linear(8, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.sigmoid(self.fc3(x))
        return x

# Avalia a probabilidade do artigo se tornar uma refer√™ncia
def evaluate_article_relevance(publication_count):
    model = ArticlePredictor()
    data = torch.tensor([[publication_count]], dtype=torch.float32)
    probability = model(data).item() * 100  # Probabilidade em porcentagem

    if probability >= 70:
        descricao = "A probabilidade de este artigo se tornar uma refer√™ncia √© alta. Isso ocorre porque h√° poucas publica√ß√µes sobre o tema, o que aumenta as chances de destaque."
    elif 30 <= probability < 70:
        descricao = "A probabilidade de este artigo se tornar uma refer√™ncia √© moderada. O tema tem uma quantidade equilibrada de publica√ß√µes, o que mant√©m as chances de destaque em um n√≠vel intermedi√°rio."
    else:
        descricao = "A probabilidade de este artigo se tornar uma refer√™ncia √© baixa. H√° muitas publica√ß√µes sobre o tema, o que reduz as chances de destaque."

    return round(probability, 2), descricao

# Fun√ß√£o para extrair texto de um arquivo PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text += page.extract_text() or ""
    return text.strip()

# Fun√ß√£o para identificar o tema principal do artigo
def identify_theme(user_text):
    words = re.findall(r'\b\w+\b', user_text)
    keywords = [word.lower() for word in words if word.lower() not in STOP_WORDS]
    keyword_freq = Counter(keywords).most_common(10)
    return ", ".join([word for word, freq in keyword_freq])

# Fun√ß√£o para gerar relat√≥rio detalhado
def generate_report(suggested_phrases, top_keywords, tema, probabilidade, descricao, monthly_counts, proportion_per_100, output_path="report.pdf"):
    doc = SimpleDocTemplate(output_path, pagesize=A4)
    styles = getSampleStyleSheet()

    justified_style = ParagraphStyle(
        'Justified',
        parent=styles['BodyText'],
        alignment=4,  # Alinhamento justificado
        spaceAfter=10,
    )

    content = [
        Paragraph("<b>Relat√≥rio de Sugest√£o de Melhorias no Artigo - CitatIA - PEAS.Co</b>", styles['Title']),
        Paragraph(f"<b>Tema Identificado com base nas principais palavras do artigo:</b> {tema}", justified_style),
        Paragraph(f"<b>Probabilidade do artigo ser uma refer√™ncia:</b> {probabilidade}%", justified_style),
        Paragraph(f"<b>Explica√ß√£o:</b> {descricao}", justified_style)
    ]

    content.append(Paragraph("<b>Estat√≠sticas de Publica√ß√µes:</b>", styles['Heading3']))
    content.append(Paragraph("<b>Publica√ß√µes de artigos com mesmo tema:</b>", justified_style))
    for month, count in monthly_counts.items():
        content.append(Paragraph(f"‚Ä¢ {month}: {count} publica√ß√µes", justified_style))
    content.append(Paragraph(f"<b>Propor√ß√£o de publica√ß√µes a cada 100 artigos:</b> {proportion_per_100:.2f}%", justified_style))

    content.append(Paragraph("<b>Artigos mais acessados, baixados e/ou citados com base no tema:</b>", styles['Heading3']))
    if suggested_phrases:
        for item in suggested_phrases:
            content.append(Paragraph(f"‚Ä¢ {item['phrase']}<br/><b>DOI:</b> {item['doi']}<br/><b>Link:</b> {item['link']}<br/><b>Cita√ß√µes:</b> {item.get('citationCount', 'N/A')}", justified_style))

    content.append(Paragraph("<b>Palavras-chave mais citadas nos artigos mais acessados:</b>", styles['Heading3']))
    if top_keywords:
        for word in top_keywords:
            content.append(Paragraph(f"‚Ä¢ {word}", justified_style))
    else:
        content.append(Paragraph("Nenhuma palavra-chave relevante encontrada.", justified_style))

    doc.build(content)

# Interface com Streamlit
def main():
    st.title("CitatIA - Potencializador de Artigos - PEAS.Co")
    
    # Registro de usu√°rio
    st.subheader("üìã Registro de Usu√°rio")
    nome = st.text_input("Nome completo")
    email = st.text_input("E-mail")
    if st.button("Salvar Dados"):
        if nome and email:
            codigo_verificacao = gerar_codigo_verificacao(email)
            salvar_email_google_sheets(nome, email, codigo_verificacao)
            st.success(f"C√≥digo de verifica√ß√£o gerado: **{codigo_verificacao}**")
        else:
            st.warning("‚ö†Ô∏è Por favor, preencha todos os campos.")

    # Upload do PDF
    uploaded_file = st.file_uploader("Envie o arquivo PDF", type='pdf')
    if uploaded_file:
        with open("uploaded_article.pdf", "wb") as f:
            f.write(uploaded_file.getbuffer())
        st.info("üîç Analisando o arquivo...")

        user_text = extract_text_from_pdf("uploaded_article.pdf")
        tema = identify_theme(user_text)

        # Buscando artigos e frases populares com base no tema identificado
        suggested_phrases = get_popular_phrases(tema, limit=10)
        # Extrair as 10 palavras mais importantes dos artigos
        top_keywords = extract_top_keywords(suggested_phrases)
        # Calculando a probabilidade com base nas refer√™ncias encontradas
        publication_count = len(suggested_phrases)
        probabilidade, descricao = evaluate_article_relevance(publication_count)
        # Gerar estat√≠sticas de publica√ß√µes
        monthly_counts, proportion_per_100 = get_publication_statistics(publication_count)

        st.success(f"‚úÖ Tema identificado: {tema}")
        st.write(f"üìà Probabilidade do artigo ser uma refer√™ncia: {probabilidade}%")
        st.write(f"‚ÑπÔ∏è {descricao}")
        st.write("<b>Estat√≠sticas de Publica√ß√µes:</b>", unsafe_allow_html=True)
        for month, count in monthly_counts.items():
            st.write(f"‚Ä¢ {month}: {count} publica√ß√µes")
        st.write(f"<b>Propor√ß√£o de publica√ß√µes a cada 100 artigos:</b> {proportion_per_100:.2f}%", unsafe_allow_html=True)
        st.write("<b>Palavras-chave mais citadas:</b>", unsafe_allow_html=True)
        if top_keywords:
            for word in top_keywords:
                st.write(f"‚Ä¢ {word}")
        else:
            st.write("Nenhuma palavra-chave relevante encontrada.")

        # Gerar e exibir link para download do relat√≥rio
        generate_report(suggested_phrases, top_keywords, tema, probabilidade, descricao, monthly_counts, proportion_per_100)
        with open("report.pdf", "rb") as file:
            st.download_button("üì• Baixar Relat√≥rio", file, "report.pdf")

    # Verifica√ß√£o de c√≥digo
    st.header("Verificar Autenticidade")
    codigo_digitado = st.text_input("Digite o c√≥digo de verifica√ß√£o:")
    if st.button("Verificar C√≥digo"):
        if verificar_codigo_google_sheets(codigo_digitado):
            st.success("‚úÖ Documento Aut√™ntico e Original!")
        else:
            st.error("‚ùå C√≥digo inv√°lido ou documento falsificado.")

    # Texto explicativo ao final da p√°gina
    st.markdown("""
    ---
    Powered By - PEAS.Co
    """)


# --- Se√ß√£o de Propaganda ---
# Incorpora√ß√£o de website (exemplo de iframe para propaganda)
st.markdown(
    "<h3><a href='https://peas8810.hotmart.host/product-page-1f2f7f92-949a-49f0-887c-5fa145e7c05d' target='_blank'>"
    "T√©cnica PROATIVA: Domine a Cria√ß√£o de Comandos Poderosos na IA e gere produtos monetiz√°veis"
    "</a></h3>",
    unsafe_allow_html=True
)
st.components.v1.iframe("https://pay.hotmart.com/U99934745U?off=y2b5nihy&hotfeature=51&_hi=eyJjaWQiOiIxNzQ4Mjk4OTUxODE2NzQ2NTc3ODk4OTY0NzUyNTAwIiwiYmlkIjoiMTc0ODI5ODk1MTgxNjc0NjU3Nzg5ODk2NDc1MjUwMCIsInNpZCI6ImM4OTRhNDg0MzJlYzRhZTk4MTNjMDJiYWE2MzdlMjQ1In0=.1748375599003&bid=1748375601381", height=250)


if __name__ == "__main__":
    main()




# TODO: Fun√ß√µes de TotalIA (preprocess_text, analyze_text_roberta, calculate_entropy, analyze_text, extract_text_from_pdf, generate_pdf_report)
# üîó URL da API gerada no Google Sheets
URL_GOOGLE_SHEETS = "https://script.google.com/macros/s/AKfycbyTpbWDxWkNRh_ZIlHuAVwZaCC2ODqTmo0Un7ZDbgzrVQBmxlYYKuoYf6yDigAPHZiZ/exec"

# =============================
# üìã Fun√ß√£o para Salvar E-mails no Google Sheets
# =============================
def salvar_email_google_sheets(nome, email, codigo="N/A"):
    dados = {"nome": nome, "email": email, "codigo": codigo}
    try:
        headers = {'Content-Type': 'application/json'}
        response = requests.post(URL_GOOGLE_SHEETS, json=dados, headers=headers)
        if response.text.strip() == "Sucesso":
            st.success("‚úÖ Seus dados foram registrados com sucesso!")
        else:
            st.error(f"‚ùå Falha ao salvar no Google Sheets: {response.text}")
    except Exception as e:
        st.error(f"‚ùå Erro na conex√£o com o Google Sheets: {e}")

# =============================
# üíæ Carregamento do Modelo Roberta (recurso pesado)
# =============================
@st.cache_resource
def load_model():
    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
    model = RobertaForSequenceClassification.from_pretrained('roberta-base')
    return tokenizer, model

try:
    tokenizer, model = load_model()
except Exception as e:
    st.error(f"Falha ao carregar o modelo Roberta: {e}")
    st.stop()

# =============================
# üîß Fun√ß√µes de An√°lise de Texto
# =============================
@st.cache_data
def preprocess_text(text: str) -> str:
    text = text.lower()
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    return re.sub(r'\s+', ' ', text).strip()

def analyze_text_roberta(text: str) -> float:
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    outputs = model(**inputs)
    prob = torch.softmax(outputs.logits, dim=1)[0, 1].item()
    return prob * 100

def calculate_entropy(text: str) -> float:
    probs = np.array([text.count(c) / len(text) for c in set(text)])
    return -np.sum(probs * np.log2(probs))

def analyze_text(text: str) -> dict:
    clean = preprocess_text(text)
    entropy = calculate_entropy(clean)
    roberta_score = analyze_text_roberta(clean)
    final_score = (roberta_score * 0.7) + (100 * (1 - entropy / 6) * 0.3)
    return {
        'IA (estimada)': f"{final_score:.2f}%",
        'Entropia': f"{entropy:.2f}",
        'Roberta (IA)': f"{roberta_score:.2f}%"
    }

# =============================
# üìÑ Fun√ß√µes de PDF (com encoding)
# =============================
def extract_text_from_pdf(pdf_file) -> str:
    text = ""
    with pdfplumber.open(io.BytesIO(pdf_file.read())) as pdf:
        for page in pdf.pages:
            text += page.extract_text() or ""
    return text

class PDFReport(FPDF):
    def _encode(self, txt: str) -> str:
        # substitui en-dash e em-dash por h√≠fen simples
        txt = txt.replace('‚Äì', '-').replace('‚Äî', '-')
        try:
            return txt.encode('latin-1', 'replace').decode('latin-1')
        except Exception:
            return ''.join(c if ord(c) < 256 else '-' for c in txt)

    def header(self):
        title = self._encode('Relat√≥rio TotalIA - PEAS.Co')
        self.set_font('Arial', 'B', 16)
        self.cell(0, 10, title, ln=True, align='C')
        self.ln(5)

    def add_results(self, results: dict):
        self.set_font('Arial', '', 12)
        for k, v in results.items():
            line = f"{k}: {v}"
            self.cell(0, 8, self._encode(line), ln=True)
        self.ln(5)

def generate_pdf_report(results: dict) -> str:
    pdf = PDFReport()
    pdf.add_page()

    # Introdu√ß√£o
    intro = 'Este relat√≥rio apresenta uma estimativa sobre a probabilidade de o texto ter sido gerado por IA.'
    pdf.set_font('Arial', '', 12)
    pdf.multi_cell(0, 8, pdf._encode(intro))
    pdf.ln(5)

    # Resultados num√©ricos
    pdf.add_results(results)

    # Explica√ß√£o detalhada da Avalia√ß√£o Roberta
    roberta_value = results['Roberta (IA)']
    pdf.ln(10)
    pdf.set_font('Arial', 'B', 14)
    pdf.cell(0, 10, pdf._encode('O que √© a "Avalia√ß√£o Roberta (Confiabilidade IA)"?'), ln=True)
    pdf.ln(2)

    explanation = (
        f"A 'Avalia√ß√£o Roberta (Confiabilidade IA)' representa a pontua√ß√£o gerada pelo modelo RoBerta "
        f"para indicar a probabilidade de que um texto tenha sido escrito por IA. "
        f"No seu relat√≥rio, o modelo atribuiu {roberta_value}.\n\n"
        "Como funciona o RoBerta:\n"
        "O RoBerta (Robustly optimized BERT approach) √© um modelo de NLP da Meta (Facebook AI), treinado "
        "com grandes volumes de texto para an√°lises sem√¢nticas profundas.\n\n"
        "Crit√©rios avaliados:\n"
        " - Coes√£o textual: IA costuma seguir padr√µes previs√≠veis.\n"
        " - Uso de conectores: express√µes como 'Portanto', 'Al√©m disso' s√£o frequentes.\n"
        " - Frases gen√©ricas: constru√ß√£o sofisticada, por√©m superficial.\n"
        " - Padr√µes lingu√≠sticos: falta de nuances humanas (ironias, ambiguidade).\n\n"
        
        " - Interpreta√ß√£o do valor - Entropia:\n"
        "0% - 3%    Alta probabilidade de IA (muito prov√°vel que o texto seja gerado por um modelo de linguagem como GPT, Bard, etc.)\n"
        "3% - 6%    Baixa probabilidade de IA (provavelmente texto humano)\n"
        
        " - Interpreta√ß√£o do valor - Roberta:\n"
        "0% - 30%    Baixa probabilidade de IA (provavelmente texto humano)\n"
        "30% - 60%   √Årea de incerteza (o texto pode conter partes geradas por IA ou apenas seguir um padr√£o formal)\n"
        "60% - 100%  Alta probabilidade de IA (muito prov√°vel que o texto seja gerado por um modelo de linguagem como GPT, Bard, etc.)"
    )
    pdf.set_font('Arial', '', 12)
    pdf.multi_cell(0, 8, pdf._encode(explanation))

    filename = "relatorio_IA.pdf"
    pdf.output(filename, 'F')
    return filename


# =============================
# üñ•Ô∏è Interface Streamlit
# =============================
st.title("üîç TotalIA - Detec√ß√£o de Texto Escrito por IA - PEAS.Co")
st.write("Fa√ßa o upload de um PDF para an√°lise:")

uploaded = st.file_uploader("Escolha um arquivo PDF", type="pdf")
if uploaded:
    texto = extract_text_from_pdf(uploaded)
    resultados = analyze_text(texto)

    st.subheader("üîé Resultados da An√°lise")
    for key, val in resultados.items():
        st.write(f"**{key}:** {val}")

    report_path = generate_pdf_report(resultados)
    with open(report_path, "rb") as f:
        st.download_button(
            "üì• Baixar Relat√≥rio em PDF",
            f.read(),
            "relatorio_IA.pdf",
            "application/pdf"
        )

# =============================
# üìã Registro de Usu√°rio (ao final)
# =============================
st.markdown("---")
st.subheader("üìã Registro de Usu√°rio - Cadastre-se")
nome = st.text_input("Nome completo", key="nome")
email = st.text_input("E-mail", key="email")
if st.button("Registrar meus dados"):
    if nome and email:
        salvar_email_google_sheets(nome, email)
    else:
        st.warning("‚ö†Ô∏è Preencha ambos os campos antes de registrar.")

# --- Se√ß√£o de Propaganda ---

  # Incorpora√ß√£o de website (exemplo de iframe para propaganda)
st.markdown(
    "<h3><a href='https://peas8810.hotmart.host/product-page-1f2f7f92-949a-49f0-887c-5fa145e7c05d' target='_blank'>"
    "T√©cnica PROATIVA: Domine a Cria√ß√£o de Comandos Poderosos na IA e gere produtos monetiz√°veis"
    "</a></h3>",
    unsafe_allow_html=True
)
st.components.v1.iframe("https://pay.hotmart.com/U99934745U?off=y2b5nihy&hotfeature=51&_hi=eyJjaWQiOiIxNzQ4Mjk4OTUxODE2NzQ2NTc3ODk4OTY0NzUyNTAwIiwiYmlkIjoiMTc0ODI5ODk1MTgxNjc0NjU3Nzg5ODk2NDc1MjUwMCIsInNpZCI6ImM4OTRhNDg0MzJlYzRhZTk4MTNjMDJiYWE2MzdlMjQ1In0=.1748375599003&bid=1748375601381", height=250)



# TODO: Fun√ß√µes de LaTeX (salvar_contato, carregar_templates, gerar_tex)
# üîó URL da API gerada no Google Sheets
URL_GOOGLE_SHEETS = "https://script.google.com/macros/s/AKfycbyTpbWDxWkNRh_ZIlHuAVwZaCC2ODqTmo0Un7ZDbgzrVQBmxlYYKuoYf6yDigAPHZiZ/exec"

# =============================
# üìã Fun√ß√£o para Salvar Nome e E-mail no Google Sheets
# =============================
def salvar_contato_google_sheets(nome, email):
    dados = {
        "nome": nome,
        "email": email
    }
    try:
        headers = {'Content-Type': 'application/json'}
        response = requests.post(URL_GOOGLE_SHEETS, json=dados, headers=headers)
        if response.text.strip() == "Sucesso":
            st.success("‚úÖ Dados registrados com sucesso!")
        else:
            st.error(f"‚ùå Erro ao salvar no Google Sheets: {response.text}")
    except Exception as e:
        st.error(f"‚ùå Erro na conex√£o com Google Sheets: {e}")

# Configura√ß√£o de diret√≥rios
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
TEMPLATES_DIR = os.path.join(BASE_DIR, "templates")

# Garante que Pandoc esteja dispon√≠vel
try:
    pypandoc.get_pandoc_version()
except (OSError, RuntimeError):
    pypandoc.download_pandoc()

# Inicializa Jinja2
env = Environment(loader=FileSystemLoader(TEMPLATES_DIR), autoescape=False)

# T√≠tulo da aplica√ß√£o
st.title("Gerador de Artigo no Padr√£o LaTex - PEAS.Co")

# --- Coleta de Nome e E-mail no Cabe√ßalho ---
st.subheader("Registre seu nome e e-mail")
nome_coleta = st.text_input("Nome:")
email_coleta = st.text_input("E-mail:")
if st.button("Enviar Dados"):
    salvar_contato_google_sheets(nome_coleta, email_coleta)

# Sele√ß√£o de template
article_type = st.selectbox(
    "Tipo de Artigo:",
    ["Estudo de Caso", "Revis√£o Bibliogr√°fica"]
)
template_file = (
    "estudo_caso.tex.j2" if article_type == "Estudo de Caso"
    else "revisao_bibliografica.tex.j2"
)
tpl = env.get_template(template_file)

# Modo de edi√ß√£o manual
manual_mode = st.checkbox("Edi√ß√£o Manual")

if manual_mode:
    # Campos de edi√ß√£o manual
    titulo_pt = st.text_input("T√≠tulo em Portugu√™s")
    titulo_en = st.text_input("T√≠tulo em Ingl√™s")
    autores_input = st.text_input("Autores (separar por v√≠rgula)")
    autores = [a.strip() for a in autores_input.split(",") if a.strip()]
    email_input = st.text_input("E-mails (separar por v√≠rgula)")
    emails = [e.strip() for e in email_input.split(",") if e.strip()]
    recebido = st.text_input("Data Recebido (dd/mm/aaaa)")
    aceito = st.text_input("Data Aceito (dd/mm/aaaa)")
    abstract_pt = st.text_area("Resumo (PT)")
    keywords_pt_input = st.text_input("Palavras-chave PT (ponto e v√≠rgula)")
    keywords_pt = [k.strip() for k in keywords_pt_input.split(";") if k.strip()]
    abstract_en = st.text_area("Abstract (EN)")
    keywords_en_input = st.text_input("Keywords EN (ponto e v√≠rgula)")
    keywords_en = [k.strip() for k in keywords_en_input.split(";") if k.strip()]

    num_sec = st.number_input(
        "N√∫mero de se√ß√µes", min_value=1, max_value=20, value=1
    )
    sections = []
    for i in range(int(num_sec)):
        sec_title = st.text_input(f"T√≠tulo da Se√ß√£o {i+1}", key=f"sec_title_{i}")
        sec_content = st.text_area(f"Conte√∫do da Se√ß√£o {i+1}", key=f"sec_content_{i}")
        sections.append({"secao": sec_title, "conteudo": sec_content})

    biblio_input = st.text_area("Bibliografia (uma refer√™ncia por linha)")
    bibliografia = []
    for idx, line in enumerate(biblio_input.splitlines()):
        if line.strip():
            bibliografia.append({"citekey": f"ref{idx+1}", "texto": line.strip()})

    if st.button("Gerar LaTeX"):
        context = {
            "titulo_pt": titulo_pt,
            "titulo_en": titulo_en,
            "autores": autores,
            "email": emails,
            "data_info": [recebido, aceito],
            "abstract_pt": abstract_pt,
            "keywords_pt": keywords_pt,
            "abstract_en": abstract_en,
            "keywords_en": keywords_en,
            "sections": sections,
            "bibliografia": bibliografia
        }
        final_tex = tpl.render(**context)
        st.subheader("C√≥digo LaTeX gerado")
        st.code(final_tex, language="latex")
        st.download_button(
            "Baixar .tex", final_tex,
            file_name="manual.tex", mime="text/x-tex"
        )
else:
    # Upload e convers√£o autom√°tica
    uploaded_file = st.file_uploader(
        "Envie seu arquivo (DOCX ou PDF)", type=["docx", "pdf"]
    )
    if uploaded_file:
        with tempfile.NamedTemporaryFile(
            delete=False, suffix=os.path.splitext(uploaded_file.name)[1]
        ) as tmp:
            tmp.write(uploaded_file.getbuffer())
            tmp_path = tmp.name
        ext = os.path.splitext(tmp_path)[1][1:]
        try:
            body = pypandoc.convert_file(tmp_path, 'latex', format=ext)
        except Exception as e:
            st.error(f"Erro na convers√£o: {e}")
            body = ""

        title = os.path.splitext(uploaded_file.name)[0]
        context = {
            "titulo_pt": title,
            "titulo_en": "",
            "autores": [],
            "email": [],
            "data_info": ["", ""],
            "abstract_pt": "",
            "keywords_pt": [],
            "abstract_en": "",
            "keywords_en": [],
            "sections": [{"secao": "Conte√∫do", "conteudo": body}],
            "bibliografia": []
        }
        final_tex = tpl.render(**context)
        st.subheader("C√≥digo LaTeX gerado")
        st.code(final_tex, language="latex")
        st.download_button(
            "Baixar .tex", final_tex,
            file_name=f"{title}.tex", mime="text/x-tex"
        )

# --- Se√ß√£o de Propaganda ---

# Incorpora√ß√£o de website (exemplo de iframe para propaganda)
st.markdown(
    "<h3><a href='https://peas8810.hotmart.host/product-page-1f2f7f92-949a-49f0-887c-5fa145e7c05d' target='_blank'>"
    "T√©cnica PROATIVA: Domine a Cria√ß√£o de Comandos Poderosos na IA e gere produtos monetiz√°veis"
    "</a></h3>",
    unsafe_allow_html=True
)
st.components.v1.iframe("https://pay.hotmart.com/U99934745U?off=y2b5nihy&hotfeature=51&_hi=eyJjaWQiOiIxNzQ4Mjk4OTUxODE2NzQ2NTc3ODk4OTY0NzUyNTAwIiwiYmlkIjoiMTc0ODI5ODk1MTgxNjc0NjU3Nzg5ODk2NDc1MjUwMCIsInNpZCI6ImM4OTRhNDg0MzJlYzRhZTk4MTNjMDJiYWE2MzdlMjQ1In0=.1748375599003&bid=1748375601381", height=250)



# Interface unificada
def main():
    st.title("üîó Portal de Ferramentas PEAS.Co")
    menu = st.sidebar.radio("Selecione o programa:", [
        "üìÑ Conversor de Documentos",
        "üïµÔ∏è‚Äç‚ôÇÔ∏è PlagIA - Detec√ß√£o de Pl√°gio",
        "‚úçÔ∏è CitatIA - Sugest√£o de Cita√ß√µes",
        "ü§ñ TotalIA - An√°lise de Texto IA",
        "üìù Gerador LaTeX"
    ])

    if menu == "üìÑ Conversor de Documentos":
        st.header("üìÑ Conversor de Documentos")
        # Chama fun√ß√µes de convers√£o PDF
        # pdf_para_word()
        # juntar_pdf()
        # dividir_pdf()
        # jpg_para_pdf()

    elif menu == "üïµÔ∏è‚Äç‚ôÇÔ∏è PlagIA - Detec√ß√£o de Pl√°gio":
        st.header("üïµÔ∏è‚Äç‚ôÇÔ∏è PlagIA - Detec√ß√£o de Pl√°gio")
        # Implementar interface de PlagIA

    elif menu == "‚úçÔ∏è CitatIA - Sugest√£o de Cita√ß√µes":
        st.header("‚úçÔ∏è CitatIA - Sugest√£o de Cita√ß√µes")
        # Implementar interface de CitatIA

    elif menu == "ü§ñ TotalIA - An√°lise de Texto IA":
        st.header("ü§ñ TotalIA - An√°lise de Texto IA")
        # Implementar interface de TotalIA

    elif menu == "üìù Gerador LaTeX":
        st.header("üìù Gerador de Artigos em LaTeX")
        # Implementar interface de LaTeX

if __name__ == "__main__":
    main()
